{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3764e4d9-f342-4cde-9125-a6b4f51c064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, size: int = 5):\n",
    "        # The size of the square grid\n",
    "        self.size = size\n",
    "\n",
    "        # Define the agent and target location; randomly chosen in `reset` and updated in `step`\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "        self._target_location = np.array([-1, -1], dtype=np.int32)\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`-1}^2\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        # Dictionary maps the abstract actions to the directions on the grid\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "        }\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "        \n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "        \n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0, self.size, size=2, dtype=int\n",
    "            )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        # We use `np.clip` to make sure we don't leave the grid bounds\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "\n",
    "        # An environment is completed if and only if the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        truncated = False\n",
    "        reward = 1 if terminated else 0  # the agent is only reached at the end of the episode\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe0b371a-a108-4b20-ab2b-a70b42b9f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    id=\"gymnasium_env/GridWorld-v0\",\n",
    "    entry_point=GridWorldEnv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "238890ae-34af-412a-aa4f-d3dfb6a4ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Q-learning agent.\"\"\"\n",
    "        self.env = env\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: dict) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # Convert observation (dict) to a tuple for hashing\n",
    "        obs_tuple = tuple(obs[\"agent\"]), tuple(obs[\"target\"])\n",
    "        \n",
    "        # With probability epsilon, return a random action\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        # With probability (1 - epsilon), act greedily\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs_tuple]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: dict,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: dict,\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        # Convert observations to tuples for hashing\n",
    "        obs_tuple = tuple(obs[\"agent\"]), tuple(obs[\"target\"])\n",
    "        next_obs_tuple = tuple(next_obs[\"agent\"]), tuple(next_obs[\"target\"])\n",
    "\n",
    "        # Compute the target Q-value\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs_tuple])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs_tuple][action]\n",
    "        )\n",
    "\n",
    "        # Update the Q-value\n",
    "        self.q_values[obs_tuple][action] += self.lr * temporal_difference\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay the exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "\n",
    "# Training the agent\n",
    "def train_agent(env, agent, num_episodes=1000):\n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(obs)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            agent.update(obs, action, reward, terminated, next_obs)\n",
    "            obs = next_obs\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # Decay epsilon after each episode\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6727645-e5a1-49f3-a904-3c59b3c4da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv(size=5)\n",
    "agent = QLearningAgent(\n",
    "    env=env,\n",
    "    learning_rate=0.1,\n",
    "    initial_epsilon=1.0,\n",
    "    epsilon_decay=0.001,\n",
    "    final_epsilon=0.01,\n",
    "    discount_factor=0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3c3b5db-bd5c-4b43-9720-ffb1e95b2d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Epsilon: 0.999\n",
      "Episode 100, Epsilon: 0.8989999999999999\n",
      "Episode 200, Epsilon: 0.7989999999999998\n",
      "Episode 300, Epsilon: 0.6989999999999997\n",
      "Episode 400, Epsilon: 0.5989999999999996\n",
      "Episode 500, Epsilon: 0.49899999999999956\n",
      "Episode 600, Epsilon: 0.39899999999999947\n",
      "Episode 700, Epsilon: 0.2989999999999994\n",
      "Episode 800, Epsilon: 0.1989999999999993\n",
      "Episode 900, Epsilon: 0.0989999999999992\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "train_agent(env, agent, num_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8214ae52-66d1-466f-8ff5-6968c7c0f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_with_trained_agent(env, agent, num_episodes=10):\n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(obs)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            obs = next_obs\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "427d1d36-d621-4aaa-9c56-f2b9b839c726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: 1\n",
      "Episode 2, Total Reward: 1\n",
      "Episode 3, Total Reward: 1\n",
      "Episode 4, Total Reward: 1\n",
      "Episode 5, Total Reward: 1\n",
      "Episode 6, Total Reward: 1\n",
      "Episode 7, Total Reward: 1\n",
      "Episode 8, Total Reward: 1\n",
      "Episode 9, Total Reward: 1\n",
      "Episode 10, Total Reward: 1\n"
     ]
    }
   ],
   "source": [
    "play_with_trained_agent(env, agent, num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bae55-9f44-4227-b174-ecba5acc498d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
